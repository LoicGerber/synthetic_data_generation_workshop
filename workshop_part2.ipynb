{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5U49YmCvZC"
      },
      "source": [
        "# Workshop Context and Objectives\n",
        "\n",
        "This notebook is a hands-on workshop exercise demonstrating how to build a workflow from remote sensing and climate data to estimating missing spatio-temporal information using an analogue approach.\n",
        "\n",
        "## Context\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "Suppose we want to run a fully distributed hydrological model over the Volta River Basin. Such models require continuous daily inputs over long time periods, but in many regions observational datasets are incomplete, temporally sparse, or unavailable for recent years. To overcome this limitation, we will use a data-driven analogue method to generate the missing information needed to run the model.\n",
        "\n",
        "<div style=\"display:flex; gap:10px;\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LoicGerber/synthetic_data_generation_workshop/52acd5850ad2311637ecf5704099c15e947f975e/isohyets.png\" width=\"35%\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LoicGerber/synthetic_data_generation_workshop/52acd5850ad2311637ecf5704099c15e947f975e/dem.png\" width=\"34.8%\">\n",
        "\n",
        "</div>\n",
        "\n",
        "In this workshop, we will use GLEAM evapotranspiration data as our target variable. The goal is to learn the relationships between climate conditions and evapotranspiration from this reference dataset, and then generate synthetic images that statistically and spatially resemble GLEAM. To achieve this, we will use ERA5-Land temperature and precipitation as predictor variables, allowing us to reconstruct realistic evapotranspiration fields even when observations are missing.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 1 - Accessing, downloading, and visualizing data\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "In the first part, we focus on data acquisition and preprocessing:\n",
        "- Access remote sensing and reanalysis data directly from Google Earth Engine (GEE).\n",
        "- Download Actual Evapotranspiration (AET) from the WaPOR Level 1 product (FAO).\n",
        "- Download daily climate predictors (precipitation and temperature) from ERA5-Land.\n",
        "- Spatially subset all datasets to the Volta River Basin using HydroSHEDS geometries.\n",
        "- Convert GEE ImageCollections into local NumPy arrays for simple python handling.\n",
        "- Save and reload processed datasets efficiently using compressed .npz files for reproducibility.\n",
        "- Visualize AET, precipitation, and temperature maps for qualitative inspection.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 2 – kNN-based analogue modeling: method understanding and validation\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "This second part focuses on understanding the kNN-based analogue method through a simple, step-by-step demonstration. Following this illustrative example, the method is applied to reconstruct three full years of daily data within a validation framework, allowing for qualitative and quantitative assessment of its performance.\n",
        "- Load pre-processed NetCDF datasets of ET (target), precipitation, and temperature (predictors) clipped to the Volta Basin.\n",
        "- Ensure temporal alignment of predictors and target.\n",
        "- Optionally subset the spatial domain for faster computation.\n",
        "- Construct covariate features, including lagged predictors (*climate window*), to capture temporal dynamics.\n",
        "- Split the dataset into training and testing periods based on years.\n",
        "- Apply k-nearest neighbors (kNN) regression, where each unobserved ET image is estimated by finding the most similar historical climate “analogues.”\n",
        "- Visualize predicted versus observed ET maps for qualitative assessment.\n",
        "- Perform quantitative evaluation using Root Mean Squared Error (RMSE) over space and time.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 3 – kNN-based analogue modeling: production run for 2021–2025\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "In this final part, we move from method validation to a full production run, applying the kNN-based analogue approach to estimate daily ET for the period 2021–2025, using all available historical observations up to 2020 as training data.\n",
        "\n",
        "Key steps include:\n",
        "- Prepare training datasets from ET (target) and climate predictors (precipitation and temperature) for all available historical data.\n",
        "- Apply the kNN analogue method to generate daily ET estimates for 2021–2025.\n",
        "- Compute _analogue-based uncertainty_ for each generated day, defined as the weighted standard deviation across the k nearest analogues.\n",
        "- Visualize daily ET reconstructions and their uncertainty.\n",
        "- Save the production datasets in both NetCDF and compressed `.npz` formats.\n",
        "- Copy final outputs to Google Drive for long-term storage and reproducibility.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "By the end of this workshop, participants will be able to:\n",
        "1. Access and process geospatial datasets from GEE and local NetCDF files.\n",
        "2. Handle spatio-temporal data using `NumPy` and `xarray` efficiently.\n",
        "3. Apply masking and subsetting to focus on a specific river basin.\n",
        "4. Understand and implement a kNN-based analogue approach for estimating missing or unobserved images.\n",
        "5. Validate analogue-based reconstructions using visual diagnostics and RMSE.\n",
        "6. Apply the same method in a production setting to temporally disaggregate remote sensing products.\n",
        "7. Build reproducible workflows that integrate data acquisition, preprocessing, modeling, validation, and production runs.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iv8IDOCXXOr"
      },
      "source": [
        "___\n",
        "\n",
        "# Part 2 – kNN-based analogue modeling: method understanding and validation using pre-downloaded data\n",
        "\n",
        "In this second part, we apply an analogue method to estimate missing evapotranspiration (ET) images using climate predictors. The core idea is simple:\n",
        "\n",
        ">For a given day with unknown ET, we search the historical archive for days with similar climate conditions (analogues), and we reconstruct ET by combining their observed ET maps using kNN.\n",
        "\n",
        "Here, the predictors describe the climatic state of a day (precipitation and temperature), while ET represents the hydrological response. By identifying past days with similar predictor patterns, we assume that their ET spatial patterns are informative analogues for the target day.\n",
        "\n",
        "Before applying this method in a production context, we first demonstrate it step by step for a single day, purely for didactic purposes. We then apply the method to reconstruct three complete years of data in a validation setup, allowing for both qualitative and quantitative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxR9STa2XaQi"
      },
      "source": [
        "\n",
        "## Import libraries and define helper functions\n",
        "\n",
        "> **Note:** this block must be run, but we will not look into it in details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STKt_KXNXwBs"
      },
      "source": [
        "In this first step, we import all Python libraries required throughout the notebook. Key libraries include:\n",
        "- `NumPy` and `xarray` for numerical data handling\n",
        "- `matplotlib` for visualization\n",
        "- `scikit-learn` for kNN regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuqDZSK1CKqM"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from google.colab import files, drive\n",
        "import matplotlib.pyplot as plt\n",
        "import xarray as xr\n",
        "import gdown\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BynTobxoYsmW"
      },
      "source": [
        "### Preparing the data\n",
        "\n",
        "We define a function `prepare_data` to:\n",
        "- Construct covariates (precipitation and temperature)\n",
        "- Optionally include lagged values (`time_window`)\n",
        "- Split the dataset into training and testing sets based on `test_periods`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3SeHmqSYvXg"
      },
      "outputs": [],
      "source": [
        "def prepare_data(et_ds, pre_ds, tmax_ds, time_window, test_periods):\n",
        "    \"\"\"\n",
        "    Prepare X_train, y_train, X_test, y_test, dates_train, dates_test\n",
        "    from ET (target) and covariates (pre, tmax).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    et_ds : xarray.Dataset\n",
        "        Dataset containing the target variable (ET).\n",
        "    pre_ds : xarray.Dataset\n",
        "        Dataset containing the precipitation predictor.\n",
        "    tmax_ds : xarray.Dataset\n",
        "        Dataset containing the maximum temperature predictor.\n",
        "    time_window : int\n",
        "        Number of lag days for covariates (e.g., if 2, includes t, t-1, t-2).\n",
        "    test_periods : list of (start_date, end_date)\n",
        "        Dates defining test periods.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train, y_train, X_test, y_test, dates_train, dates_test\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # 1) EXTRACT DATAARRAYS FROM DATASETS\n",
        "    # ==========================================================\n",
        "    # Assume each dataset contains a single variable\n",
        "    target_var = list(et_ds.data_vars)[0]\n",
        "    pre_var    = list(pre_ds.data_vars)[0]\n",
        "    tmax_var   = list(tmax_ds.data_vars)[0]\n",
        "\n",
        "    # Extract DataArrays\n",
        "    et_da   = et_ds[target_var]\n",
        "    pre_da  = pre_ds[pre_var]\n",
        "    tmax_da = tmax_ds[tmax_var]\n",
        "\n",
        "    # Ensure target has a time dimension\n",
        "    if \"time\" not in et_da.dims:\n",
        "        raise ValueError(\"ET DataArray has no 'time' dimension.\")\n",
        "    time_dim = \"time\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # 2) ALIGN PREDICTORS WITH TARGET TIME AXIS\n",
        "    # ==========================================================\n",
        "    # Use ET time as reference\n",
        "    time = et_da[time_dim]\n",
        "\n",
        "    # Subset predictors to match ET dates\n",
        "    pre_da  = pre_da.sel({time_dim: time})\n",
        "    tmax_da = tmax_da.sel({time_dim: time})\n",
        "\n",
        "    # ==========================================================\n",
        "    # 3) BUILD FEATURE STACK (INCLUDING LAGGED VARIABLES)\n",
        "    # ==========================================================\n",
        "    feat_list = []\n",
        "\n",
        "    if time_window == 0:\n",
        "        # Only current-day predictors\n",
        "        feat_list.extend([pre_da, tmax_da])\n",
        "\n",
        "        # Effective ET is unchanged\n",
        "        et_eff = et_da\n",
        "\n",
        "    else:\n",
        "        # Include current day and lagged days\n",
        "        for da in (pre_da, tmax_da):\n",
        "\n",
        "            # Current day\n",
        "            feat_list.append(da)\n",
        "\n",
        "            # Add lagged predictors\n",
        "            for lag in range(1, time_window + 1):\n",
        "                feat_list.append(\n",
        "                    da.shift({time_dim: lag})\n",
        "                )\n",
        "\n",
        "        # Remove first days where lagged data is incomplete\n",
        "        et_eff = et_da.isel({time_dim: slice(time_window, None)})\n",
        "        feat_list = [\n",
        "            da.isel({time_dim: slice(time_window, None)})\n",
        "            for da in feat_list\n",
        "        ]\n",
        "\n",
        "        # Update time coordinate accordingly\n",
        "        time = et_eff[time_dim]\n",
        "\n",
        "    # Concatenate predictors along a new \"feature\" dimension\n",
        "    features = xr.concat(feat_list, dim=\"feature\")\n",
        "\n",
        "    # ==========================================================\n",
        "    # 4) CREATE TRAIN / TEST SPLIT BASED ON DATE PERIODS\n",
        "    # ==========================================================\n",
        "    # Ensure time coordinate is datetime\n",
        "    if not np.issubdtype(time.dtype, np.datetime64):\n",
        "        raise TypeError(\"time coordinate is not datetime-like.\")\n",
        "\n",
        "    # Initialize boolean mask for test periods\n",
        "    test_mask = np.zeros(time.size, dtype=bool)\n",
        "\n",
        "    # Mark all dates belonging to test periods\n",
        "    for start, end in test_periods:\n",
        "        test_mask |= (\n",
        "            (time >= np.datetime64(start)) &\n",
        "            (time <= np.datetime64(end))\n",
        "        )\n",
        "\n",
        "    # Training mask is inverse\n",
        "    train_mask = ~test_mask\n",
        "\n",
        "    # Convert masks to indices\n",
        "    train_time_idx = np.where(train_mask)[0]\n",
        "    test_time_idx  = np.where(test_mask)[0]\n",
        "\n",
        "    # Safety checks\n",
        "    if train_time_idx.size == 0:\n",
        "        raise ValueError(\"No training samples left after applying test periods.\")\n",
        "    if test_time_idx.size == 0:\n",
        "        raise ValueError(\"No test samples found for the given test periods.\")\n",
        "\n",
        "    # Subset datasets\n",
        "    features_train = features.isel({time_dim: train_time_idx})\n",
        "    features_test  = features.isel({time_dim: test_time_idx})\n",
        "\n",
        "    et_train = et_eff.isel({time_dim: train_time_idx})\n",
        "    et_test  = et_eff.isel({time_dim: test_time_idx})\n",
        "\n",
        "    # ==========================================================\n",
        "    # 5) CONVERT XARRAY → NUMPY ARRAYS\n",
        "    # ==========================================================\n",
        "    # Identify spatial dimensions (everything except time)\n",
        "    spatial_dims = [d for d in et_eff.dims if d != time_dim]\n",
        "\n",
        "    # Reorder dimensions for ML-friendly format:\n",
        "    # (time, features, lat, lon)\n",
        "    X_train = features_train.transpose(\n",
        "        time_dim, \"feature\", *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    X_test = features_test.transpose(\n",
        "        time_dim, \"feature\", *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    # Target arrays: (time, lat, lon)\n",
        "    y_train = et_train.transpose(\n",
        "        time_dim, *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    y_test = et_test.transpose(\n",
        "        time_dim, *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    # ==========================================================\n",
        "    # 6) EXTRACT DATE ARRAYS\n",
        "    # ==========================================================\n",
        "    dates_train = et_train[time_dim].values\n",
        "    dates_test  = et_test[time_dim].values\n",
        "\n",
        "    # ==========================================================\n",
        "    # RETURN RESULTS\n",
        "    # ==========================================================\n",
        "    return X_train, y_train, X_test, y_test, dates_train, dates_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNVCCQNkZH-S"
      },
      "source": [
        "### kNN estimation\n",
        "\n",
        "- For each test day, kNN identifies climate analogues in the training set.\n",
        "- ET is reconstructed from the analogue ET maps.\n",
        "- Predictions are generated independently for each day.\n",
        "- For each query day, the `k_neighbors` closest analogues are weighted by similarity (inverse distance).\n",
        "- The predicted ET is the weighted mean of these analogues.\n",
        "- The \"uncertainty\" is the weighted standard deviation of the analogue ET values around the mean.\n",
        "\n",
        "> **Note:** This uncertainty reflects the spread of the analogue ensemble not a formal statistical confidence interval.  \n",
        "> It indicates regions or days where the selected analogues are less consistent (larger spread = higher analogue-based uncertainty), not a formal statistical confidence interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXqfdXW7ZRv-"
      },
      "outputs": [],
      "source": [
        "def knn_predict_with_uncertainty(X_train, y_train, X_query, k_neighbors):\n",
        "    \"\"\"\n",
        "    KNN analogue prediction with uncertainty from analogue spread.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : (T_train, n_features, latX, lonX)\n",
        "        Predictor variables for training period\n",
        "    y_train : (T_train, latY, lonY)\n",
        "        Target variable for training period\n",
        "    X_query : (T_query, n_features, latX, lonX)\n",
        "        Predictor variables for query period\n",
        "    k_neighbors : int\n",
        "        Number of analogues (nearest neighbors)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_mean : (T_query, latY, lonY)\n",
        "        Predicted mean field\n",
        "    y_std  : (T_query, latY, lonY)\n",
        "        Analogue-based uncertainty (weighted std)\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------\n",
        "    # Extract dimensions\n",
        "    # -------------------------------\n",
        "    T_train, n_features, n_latX, n_lonX = X_train.shape\n",
        "    T_query = X_query.shape[0]\n",
        "    _, n_latY, n_lonY = y_train.shape\n",
        "\n",
        "    # Total number of spatial pixels\n",
        "    NX = n_latX * n_lonX\n",
        "    NY = n_latY * n_lonY\n",
        "\n",
        "    # ==========================================================\n",
        "    # X MASKING — remove pixels that contain NaNs in ANY time or feature\n",
        "    # ==========================================================\n",
        "\n",
        "    # Combine train + query to ensure consistent valid mask\n",
        "    X_all = np.concatenate([X_train, X_query], axis=0)\n",
        "\n",
        "    # Valid pixel mask → True where all values finite across time + features\n",
        "    valid_X = np.all(np.isfinite(X_all), axis=(0, 1))\n",
        "\n",
        "    # Flatten spatial mask for vector indexing\n",
        "    mask_X = valid_X.ravel()\n",
        "\n",
        "    # Reshape predictors → (time, features, pixels)\n",
        "    # then keep only valid pixels\n",
        "    Xtr = X_train.reshape(T_train, n_features, NX)[:, :, mask_X]\n",
        "    Xq  = X_query.reshape(T_query, n_features, NX)[:, :, mask_X]\n",
        "\n",
        "    # Flatten feature + pixel dimensions into single vector\n",
        "    # final shapes:\n",
        "    #   Xtr_vec = (T_train, n_valid_features)\n",
        "    #   Xq_vec  = (T_query, n_valid_features)\n",
        "    Xtr_vec = Xtr.reshape(T_train, -1)\n",
        "    Xq_vec  = Xq.reshape(T_query,  -1)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Y MASKING — remove pixels invalid in training target\n",
        "    # ==========================================================\n",
        "\n",
        "    # Valid target pixels across ALL training time\n",
        "    valid_Y = np.all(np.isfinite(y_train), axis=0)\n",
        "\n",
        "    # Flatten spatial mask\n",
        "    mask_Y = valid_Y.ravel()\n",
        "\n",
        "    # Reshape y_train → (time, pixels) and keep valid ones\n",
        "    ytr = y_train.reshape(T_train, NY)[:, mask_Y]\n",
        "\n",
        "    # ==========================================================\n",
        "    # NEAREST NEIGHBOUR SEARCH\n",
        "    # ==========================================================\n",
        "\n",
        "    # Build KNN search structure\n",
        "    nn = NearestNeighbors(\n",
        "        n_neighbors=k_neighbors,\n",
        "        metric=\"euclidean\",\n",
        "        n_jobs=-1              # use all CPU cores\n",
        "    )\n",
        "\n",
        "    # Fit on training predictors\n",
        "    nn.fit(Xtr_vec)\n",
        "\n",
        "    # Find k nearest analogues for each query timestep\n",
        "    distances, indices = nn.kneighbors(Xq_vec)\n",
        "\n",
        "    # ==========================================================\n",
        "    # DISTANCE → WEIGHTS\n",
        "    # ==========================================================\n",
        "\n",
        "    # Convert distances to inverse-distance weights\n",
        "    # (small distance → large weight)\n",
        "    weights = 1.0 / (distances + 1e-12)\n",
        "\n",
        "    # Normalize weights so each row sums to 1\n",
        "    weights /= weights.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # ==========================================================\n",
        "    # ANALOGUE PREDICTIONS\n",
        "    # ==========================================================\n",
        "\n",
        "    # Dimensions\n",
        "    Tq, k = indices.shape\n",
        "\n",
        "    # Container for analogue target values\n",
        "    # shape = (query_time, k_neighbors, valid_pixels)\n",
        "    y_pred = np.empty((Tq, k, ytr.shape[1]))\n",
        "\n",
        "    # Gather analogue fields\n",
        "    for i in range(Tq):\n",
        "        # indices[i] → indices of k nearest training dates\n",
        "        y_pred[i] = ytr[indices[i]]\n",
        "\n",
        "    # ==========================================================\n",
        "    # WEIGHTED MEAN PREDICTION\n",
        "    # ==========================================================\n",
        "\n",
        "    # Weighted average across k analogues\n",
        "    y_mean_flat = np.sum(\n",
        "        weights[:, :, None] * y_pred,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # ==========================================================\n",
        "    # UNCERTAINTY ESTIMATION\n",
        "    # ==========================================================\n",
        "\n",
        "    # Weighted variance across analogues\n",
        "    y_var_flat = np.sum(\n",
        "        weights[:, :, None] * (y_pred - y_mean_flat[:, None, :])**2,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Standard deviation = uncertainty estimate\n",
        "    y_std_flat = np.sqrt(y_var_flat)\n",
        "\n",
        "    # ==========================================================\n",
        "    # RESTORE FULL SPATIAL GRID (reinsert masked pixels)\n",
        "    # ==========================================================\n",
        "\n",
        "    # Initialize full grids with NaNs\n",
        "    y_mean = np.full((Tq, NY), np.nan)\n",
        "    y_std  = np.full((Tq, NY), np.nan)\n",
        "\n",
        "    # Fill valid pixels only\n",
        "    y_mean[:, mask_Y] = y_mean_flat\n",
        "    y_std[:,  mask_Y] = y_std_flat\n",
        "\n",
        "    # Reshape back to spatial maps\n",
        "    y_mean = y_mean.reshape(Tq, n_latY, n_lonY)\n",
        "    y_std  = y_std.reshape(Tq, n_latY, n_lonY)\n",
        "\n",
        "    return y_mean, y_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcECBdlFZXmz"
      },
      "source": [
        "### Visualizing Observed vs Predicted Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtuJ9zwyZX_L"
      },
      "outputs": [],
      "source": [
        "def plot_maps(y_obs, y_mean, y_std, dates, lon, lat, indices):\n",
        "    \"\"\"\n",
        "    Plot observed ET, predicted ET, error, and uncertainty maps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_obs : array (T, lat, lon)\n",
        "        Observed / reference ET (use None for production runs).\n",
        "    y_mean : array (T, lat, lon)\n",
        "        Mean reconstructed ET.\n",
        "    y_std : array (T, lat, lon)\n",
        "        Uncertainty (analogue spread).\n",
        "    dates : array-like (T,)\n",
        "        Datetime array.\n",
        "    lon, lat : 1D or 2D arrays\n",
        "        Spatial coordinates.\n",
        "    indices : list of int\n",
        "        Time indices to visualise.\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # PREPARE SPATIAL GRID\n",
        "    # ==========================================================\n",
        "    # If coordinates are 1D vectors, convert to 2D meshgrid\n",
        "    # so they match the shape required by pcolormesh.\n",
        "    if lon.ndim == 1 and lat.ndim == 1:\n",
        "        Lon, Lat = np.meshgrid(lon, lat)\n",
        "    else:\n",
        "        # Already 2D coordinate grids\n",
        "        Lon, Lat = lon, lat\n",
        "\n",
        "    # ==========================================================\n",
        "    # LOOP THROUGH REQUESTED TIME INDICES\n",
        "    # ==========================================================\n",
        "    for idx in indices:\n",
        "\n",
        "        # Extract prediction and uncertainty for this timestep\n",
        "        y_pred = y_mean[idx]\n",
        "        y_unc  = y_std[idx]\n",
        "\n",
        "        # Convert date to readable format\n",
        "        date = np.array(dates[idx]).astype(\"datetime64[D]\")\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # IF REFERENCE DATA EXISTS → compute error + shared scale\n",
        "        # ------------------------------------------------------\n",
        "        if y_obs is not None:\n",
        "\n",
        "            # Reference map\n",
        "            y_ref = y_obs[idx]\n",
        "\n",
        "            # Prediction error map\n",
        "            y_err = y_pred - y_ref\n",
        "\n",
        "            # Use same color scale for observed + predicted\n",
        "            vmin  = np.nanmin([y_ref, y_pred])\n",
        "            vmax  = np.nanmax([y_ref, y_pred])\n",
        "\n",
        "            # Error scale based on robust percentile\n",
        "            vmax_err = np.nanpercentile(np.abs(y_err), 95)\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # IF NO REFERENCE DATA (production mode)\n",
        "        # ------------------------------------------------------\n",
        "        else:\n",
        "            y_ref = None\n",
        "            y_err = None\n",
        "\n",
        "            # Scale only based on prediction\n",
        "            vmin  = np.nanmin(y_pred)\n",
        "            vmax  = np.nanmax(y_pred)\n",
        "\n",
        "        # Robust scale for uncertainty\n",
        "        vmax_std = np.nanpercentile(y_unc, 95)\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # DETERMINE NUMBER OF PANELS\n",
        "        # ------------------------------------------------------\n",
        "        # If reference exists → show 4 panels\n",
        "        # Otherwise → only prediction + uncertainty\n",
        "        ncols = 4 if y_obs is not None else 2\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(\n",
        "            1, ncols,\n",
        "            figsize=(4.5 * ncols, 4),\n",
        "            constrained_layout=True\n",
        "        )\n",
        "\n",
        "        # Column pointer for flexible plotting\n",
        "        col = 0\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 1 — OBSERVED MAP\n",
        "        # ======================================================\n",
        "        if y_obs is not None:\n",
        "\n",
        "            im0 = axes[col].pcolormesh(\n",
        "                Lon, Lat, y_ref,\n",
        "                shading=\"auto\",\n",
        "                vmin=vmin, vmax=vmax\n",
        "            )\n",
        "\n",
        "            axes[col].set_title(f\"Observed ET\\n{date}\")\n",
        "            axes[col].set_aspect(\"equal\")\n",
        "\n",
        "            # Colorbar\n",
        "            c0 = plt.colorbar(im0, ax=axes[col])\n",
        "            c0.set_label(\"[mm/day]\")\n",
        "\n",
        "            col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 2 — PREDICTED MAP\n",
        "        # ======================================================\n",
        "        im1 = axes[col].pcolormesh(\n",
        "            Lon, Lat, y_pred,\n",
        "            shading=\"auto\",\n",
        "            vmin=vmin, vmax=vmax\n",
        "        )\n",
        "\n",
        "        # Title depends on mode\n",
        "        if y_obs is not None:\n",
        "            axes[col].set_title(\"Predicted ET\")\n",
        "        else:\n",
        "            axes[col].set_title(f\"Predicted ET\\n{date}\")\n",
        "\n",
        "        axes[col].set_aspect(\"equal\")\n",
        "\n",
        "        # Colorbar\n",
        "        c1 = plt.colorbar(im1, ax=axes[col])\n",
        "        c1.set_label(\"[mm/day]\")\n",
        "\n",
        "        col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 3 — ERROR MAP (only if reference exists)\n",
        "        # ======================================================\n",
        "        if y_obs is not None:\n",
        "\n",
        "            im2 = axes[col].pcolormesh(\n",
        "                Lon, Lat, y_err,\n",
        "                shading=\"auto\",\n",
        "                vmin=-vmax_err, vmax=vmax_err,\n",
        "                cmap=\"coolwarm\"   # diverging colormap for errors\n",
        "            )\n",
        "\n",
        "            axes[col].set_title(\"Error (Pred − Ref)\")\n",
        "            axes[col].set_aspect(\"equal\")\n",
        "\n",
        "            # Colorbar\n",
        "            c2 = plt.colorbar(im2, ax=axes[col])\n",
        "            c2.set_label(\"[mm/day]\")\n",
        "\n",
        "            col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # FINAL PANEL — UNCERTAINTY MAP\n",
        "        # ======================================================\n",
        "        im3 = axes[col].pcolormesh(\n",
        "            Lon, Lat, y_unc,\n",
        "            shading=\"auto\",\n",
        "            vmin=0, vmax=vmax_std,\n",
        "            cmap=\"magma\"      # sequential colormap for uncertainty\n",
        "        )\n",
        "\n",
        "        axes[col].set_title(\"Uncertainty (analogue spread)\")\n",
        "        axes[col].set_aspect(\"equal\")\n",
        "\n",
        "        # Colorbar\n",
        "        c3 = plt.colorbar(im3, ax=axes[col])\n",
        "        c3.set_label(\"[mm/day]\")\n",
        "\n",
        "        # ======================================================\n",
        "        # DISPLAY FIGURE\n",
        "        # ======================================================\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WgVtcWMZ1yZ"
      },
      "source": [
        "### Quantitative evaluation using RMSE\n",
        "\n",
        "- Here, we compute Root Mean Squared Error (RMSE):\n",
        "- RMSE $= \\sqrt{\\frac{1}{N}\\sum^{N}_{i=1}\\left(y_{obs} - y_{pred}\\right)^2}$\n",
        "- Computed spatially per time step.\n",
        "- Can also be aggregated over the entire test period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72CvYrd3Z2PA"
      },
      "outputs": [],
      "source": [
        "def compute_rmse(y_obs, y_pred, dates, max_gap_days=1):\n",
        "    \"\"\"\n",
        "    Compute RMSE over space for each time step and split by date gaps.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rmse : (T,)\n",
        "    segments : list of index arrays for continuous date segments\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # INITIALIZATION\n",
        "    # ==========================================================\n",
        "    # Number of time steps\n",
        "    T = y_obs.shape[0]\n",
        "\n",
        "    # Array to store RMSE at each timestep\n",
        "    rmse = np.zeros(T)\n",
        "\n",
        "    # ==========================================================\n",
        "    # COMPUTE SPATIAL RMSE PER TIME STEP\n",
        "    # ==========================================================\n",
        "    for t in range(T):\n",
        "\n",
        "        # Difference map between observation and prediction\n",
        "        diff = y_obs[t] - y_pred[t]\n",
        "\n",
        "        # RMSE over spatial domain (ignores NaNs)\n",
        "        rmse[t] = np.sqrt(np.nanmean(diff**2))\n",
        "\n",
        "    # ==========================================================\n",
        "    # DETECT TEMPORAL GAPS IN THE DATE SERIES\n",
        "    # ==========================================================\n",
        "    # Convert dates to numpy array for vectorized operations\n",
        "    dates = np.asarray(dates)\n",
        "\n",
        "    # Compute day differences between consecutive dates\n",
        "    gaps = np.diff(dates).astype(\"timedelta64[D]\").astype(int)\n",
        "\n",
        "    # Identify where gaps exceed allowed threshold\n",
        "    breaks = np.where(gaps > max_gap_days)[0]\n",
        "\n",
        "    # ==========================================================\n",
        "    # SPLIT TIME SERIES INTO CONTINUOUS SEGMENTS\n",
        "    # ==========================================================\n",
        "    segments = []\n",
        "\n",
        "    # Start index of current segment\n",
        "    start = 0\n",
        "\n",
        "    # Loop over detected breaks\n",
        "    for b in breaks:\n",
        "\n",
        "        # Segment runs from current start → break index\n",
        "        segments.append(np.arange(start, b + 1))\n",
        "\n",
        "        # Next segment starts after the break\n",
        "        start = b + 1\n",
        "\n",
        "    # Add final segment after last break\n",
        "    segments.append(np.arange(start, T))\n",
        "\n",
        "    # ==========================================================\n",
        "    # RETURN RESULTS\n",
        "    # ==========================================================\n",
        "    return rmse, segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuVM8PxgDaIa"
      },
      "source": [
        "## 2.1 - Why we switch to pre-downloaded datasets\n",
        "\n",
        "In the previous section, we demonstrated how to download data directly from Google Earth Engine using a short time window.\n",
        "\n",
        "**Important practical note:** Downloading large spatial domains or long time periods from GEE can be **very slow** and sometimes unstable in a workshop setting.\n",
        "\n",
        "For this reason, we now switch to pre-downloaded datasets stored on a shared Google Drive. This allows us to:\n",
        "- Work with a much larger temporal dataset\n",
        "- Ensure that all participants use identical data\n",
        "- Focus on the methodology rather than data access\n",
        "\n",
        "## 2.2 - Pre-downloaded datasets used in this workshop\n",
        "\n",
        "In this second part, we use the following datasets:\n",
        "\n",
        "**Target**:\n",
        "- GLEAM Actual Evapotranspiration (ET)\n",
        "\n",
        "GLEAM, the target, has a spatial resolution of 0.25° and is available daily from 1980 to 2020. It provides a long, continuous reference dataset, which makes quantitative validation of the analogue reconstructions possible.\n",
        "\n",
        "**Predictors**\n",
        "- ERA5-Land precipitation (PRE)\n",
        "- ERA5-Land maximum daily temperature (TMAX)\n",
        "\n",
        "The ERA5-Land reanalysis dataset provides daily maps as a spatial resolution of 0.1° from 1940 to today.\n",
        "\n",
        "Using ERA5-Land predictors together with GLEAM ET allows us to:\n",
        "- Build analogues using a long climatological archive\n",
        "- Reconstruct ET for selected days\n",
        "- Directly compare reconstructed ET maps against the reference GLEAM ET, enabling objective performance assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrsisB4xLnSM"
      },
      "outputs": [],
      "source": [
        "url_et   = \"https://drive.google.com/file/d/1QKJCwt44LdsQNpNBIfFA4-kFs2MTaJRD/view?usp=drive_link\"\n",
        "url_pre  = \"https://drive.google.com/file/d/1ZyRFuYSFle5zqPpyoSBXLiyIp5frSCWk/view?usp=drive_link\"\n",
        "url_tmax = \"https://drive.google.com/file/d/1Ru0eKa_DrvjA2x_F1MF7EPixge2VXIJh/view?usp=drive_link\"\n",
        "\n",
        "gdown.download(url_et,   output=\"et.nc\",   fuzzy=True)\n",
        "gdown.download(url_pre,  output=\"pre.nc\",  fuzzy=True)\n",
        "gdown.download(url_tmax, output=\"tmax.nc\", fuzzy=True)\n",
        "\n",
        "et   = xr.open_dataset(\"et.nc\")\n",
        "pre  = xr.open_dataset(\"pre.nc\")\n",
        "tmax = xr.open_dataset(\"tmax.nc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDl5_1u711k2"
      },
      "source": [
        "We can interogate `pre` to see how the dataset is constructed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI_Is5330Z_B"
      },
      "outputs": [],
      "source": [
        "pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8GzFXwXG6m"
      },
      "source": [
        "We can also visualise the total daily precipitation over the region of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJMQNLvTXQDH"
      },
      "outputs": [],
      "source": [
        "tot_pre = pre[\"pre\"].sum(dim=(\"longitude\", \"latitude\"))\n",
        "tot_pre.plot()\n",
        "plt.show()\n",
        "del tot_pre\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzYRYO-XxTLF"
      },
      "source": [
        "Or the time-series for only one pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1lxoSwUwpo-"
      },
      "outputs": [],
      "source": [
        "ts = pre[\"pre\"].isel(longitude=50, latitude=50)  # longitude and latitude selection based on pixel number, not actual degrees.\n",
        "ts.plot()\n",
        "plt.show()\n",
        "del ts\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8sh-GdUHG0w"
      },
      "source": [
        "## 2.3 - Temporal alignment of predictors and target\n",
        "\n",
        "We first restrict all datasets to a common time window (here 2000–2020).\n",
        "This reduces the amount of data loaded in memory while retaining enough temporal variability for analogue selection.\n",
        "\n",
        "Even after subsetting, small differences in time coordinates may remain (e.g. missing days or different calendar handling). To ensure perfect temporal consistency, precipitation and temperature are explicitly aligned to the ET time axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XO1rTzNQwk9"
      },
      "outputs": [],
      "source": [
        "t_start = \"2000-01-01\"\n",
        "t_end   = \"2020-12-31\"\n",
        "\n",
        "et   =   et.sel(time=slice(t_start, t_end))\n",
        "pre  =  pre.sel(time=slice(t_start, t_end))\n",
        "tmax = tmax.sel(time=slice(t_start, t_end))\n",
        "\n",
        "pre  =  pre.sel(time=et.time)\n",
        "tmax = tmax.sel(time=et.time)\n",
        "\n",
        "# use the same names for spatial coordinates\n",
        "pre  =  pre.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
        "tmax = tmax.rename({'longitude': 'lon', 'latitude': 'lat'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVO00ViJ2Aqf"
      },
      "source": [
        "We can interrogate `pre` to see how the dataset is constructed. Notice that the time availability is now shorter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD6kP5Hf1px1"
      },
      "outputs": [],
      "source": [
        "pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTfYBXYTIPTc"
      },
      "source": [
        "## 2.5 - Preparing the data\n",
        "Now we call the function to prepare our datasets. We define here the `time_window` length and the `n_test_years` number of years to reconstruct for validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RE-Io__Rq3m"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test, dates_train, dates_test = prepare_data(\n",
        "    et_ds         = et,\n",
        "    pre_ds        = pre,\n",
        "    tmax_ds       = tmax,\n",
        "    time_window   = 2,    # <------ NUMBER OF DAYS IN CLIMATE WINDOW (min 1 day): Here one day lag\n",
        "    test_periods  = [\n",
        "        (\"2018-01-01\", \"2018-12-31\"),     # <------ VALIDATION DATES\n",
        "        (\"2020-06-01\", \"2020-08-31\")      # <------ ADDITIONAL VALIDATION DATES\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwKOW_a0qyCX"
      },
      "source": [
        "## 2.6 - Analogue selection for a single day (illustrative example)\n",
        "\n",
        "Before applying kNN to the full testing period, we first illustrate the analogue-based reconstruction for one specific day.\n",
        "\n",
        "This example helps to explicitly visualize:\n",
        "- The predictor fields for the target day\n",
        "- The dates of the selected analogues\n",
        "- The ET maps of these analogue days\n",
        "- The reconstructed ET map\n",
        "- The reference (observed) ET map for comparison\n",
        "\n",
        "<div style=\"display:flex; gap:10px;\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LoicGerber/synthetic_data_generation_workshop/52acd5850ad2311637ecf5704099c15e947f975e/knn.png\" width=\"75%\">\n",
        "\n",
        "</div>\n",
        "\n",
        "### 2.6.1 Selecting a target day\n",
        "\n",
        "We select one day from the test period and treat its ET map as _missing_.\n",
        "Only the predictors (precipitation and temperature) for that day are used to find analogues.\n",
        "\n",
        "Conceptually:\n",
        "- Input: PRE(t), TMAX(t)\n",
        "- Output: reconstructed ET(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVU5qt1wrCwB"
      },
      "outputs": [],
      "source": [
        "# --- Select one target day from the test set ---\n",
        "target_idx = 0            # <--------------------------------- CHANGE TO ANY INDEX IN [0, len(dates_test)-1]\n",
        "\n",
        "target_date = dates_test[target_idx]\n",
        "print(f\"Target reconstruction date: {np.datetime_as_string(target_date, unit='D')}\")\n",
        "\n",
        "# Extract predictors and reference ET for that day\n",
        "X_target     = X_test[target_idx:target_idx+1]  # shape (1, n_features, lat, lon)\n",
        "y_target_ref = y_test[target_idx]               # shape (lat, lon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvawTTNQrDTY"
      },
      "source": [
        "### 2.6.2 Visualizing the predictors for the selected day\n",
        "\n",
        "For the selected day, we first visualize the predictor fields:\n",
        "- Precipitation\n",
        "- Maximum temperature\n",
        "\n",
        "This step allows us to physically interpret what _similarity_ means in the analogue space and ensures that the climate situation of the target day is well understood before reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J4HduIyrKEv"
      },
      "outputs": [],
      "source": [
        "# --- Extract predictor fields ---\n",
        "# Feature order:\n",
        "# [pre(t-2), pre(t-1), pre(t), tmax(t-2), tmax(t-1), tmax(t)]\n",
        "\n",
        "pre_t2, pre_t1, pre_t0    = X_target[0, 0], X_target[0, 1], X_target[0, 2]\n",
        "tmax_t2, tmax_t1, tmax_t0 = X_target[0, 3], X_target[0, 4], X_target[0, 5]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(8, 10), constrained_layout=True)\n",
        "\n",
        "plots = [\n",
        "    (pre_t0,  \"Precipitation (t)\",   0, 0, \"Precipitation [mm/day]\"),\n",
        "    (pre_t1,  \"Precipitation (t-1)\", 1, 0, \"Precipitation [mm/day]\"),\n",
        "    (pre_t2,  \"Precipitation (t-2)\", 2, 0, \"Precipitation [mm/day]\"),\n",
        "    (tmax_t0, \"Tmax (t)\",            0, 1, \"Tmax [°C]\"),\n",
        "    (tmax_t1, \"Tmax (t-1)\",          1, 1, \"Tmax [°C]\"),\n",
        "    (tmax_t2, \"Tmax (t-2)\",          2, 1, \"Tmax [°C]\"),\n",
        "]\n",
        "\n",
        "for data, title, row, col, cbar_label in plots:\n",
        "    ax = axes[row, col]\n",
        "    im = ax.imshow(data, origin=\"upper\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(cbar_label)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2vawTGdrKX0"
      },
      "source": [
        "### 2.6.3 Selecting analogue days using kNN\n",
        "\n",
        "Using the kNN algorithm:\n",
        "- Each day in the training period is represented as a high-dimensional vector of predictor values (all grid cells and variables).\n",
        "- Here, distances are computed as Euclidean distances between flattened (collapsed into one dimension) predictor maps, meaning that each day is represented as a high-dimensional vector combining all predictor values over space and time lags.\n",
        "- The k nearest neighbors correspond to the most similar climate situations, i.e. the analogues.\n",
        "\n",
        "For transparency, we explicitly list:\n",
        "- The dates of the selected analogue days\n",
        "- Their distance-based ranking\n",
        "\n",
        "This step emphasizes that kNN is used purely for analogue selection, not as a black-box regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHzyqAOLrWS7"
      },
      "outputs": [],
      "source": [
        "# --- Flatten predictors exactly as in knn_predict ---\n",
        "T_train, n_features, n_lat, n_lon = X_train.shape\n",
        "NX = n_lat * n_lon\n",
        "\n",
        "# Mask valid X space (same logic as knn_predict)\n",
        "X_all = np.concatenate([X_train, X_test], axis=0)\n",
        "valid_X_space = np.all(np.isfinite(X_all), axis=(0, 1))\n",
        "mask_X_flat = valid_X_space.ravel()\n",
        "\n",
        "X_train_flat = X_train.reshape(T_train, n_features, NX)[:, :, mask_X_flat]\n",
        "X_target_flat = X_target.reshape(1, n_features, NX)[:, :, mask_X_flat]\n",
        "\n",
        "X_train_vec = X_train_flat.reshape(T_train, -1)\n",
        "X_target_vec = X_target_flat.reshape(1, -1)\n",
        "\n",
        "# --- Compute distances to all training days ---\n",
        "distances = pairwise_distances(X_train_vec, X_target_vec, metric=\"euclidean\").ravel()\n",
        "\n",
        "k = 5           # <-------------------------------------------------------------------- NUMBER OF ANALOGUES TO DISPLAY\n",
        "analogue_idx = np.argsort(distances)[:k]\n",
        "\n",
        "analogue_dates = dates_train[analogue_idx]\n",
        "\n",
        "print(\"Selected analogue dates:\")\n",
        "for i, d in zip(analogue_idx, analogue_dates):\n",
        "    print(f\"  {np.datetime_as_string(d, unit='D')}  | distance = {distances[i]:.3f}\")\n",
        "\n",
        "# --- FREE RAM ---\n",
        "del X_all, X_train_flat, X_target_flat, X_train_vec, X_target_vec, valid_X_space, mask_X_flat\n",
        "gc.collect();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaH4fgUerWpj"
      },
      "source": [
        "### 2.6.4 Visualizing ET maps of the selected analogues\n",
        "\n",
        "Next, we plot the ET maps corresponding to the selected analogue dates.\n",
        "\n",
        "This allows us to verify that:\n",
        "- The analogue ET patterns are physically plausible\n",
        "- Spatial structures are consistent across analogue days\n",
        "- Variability between analogues is preserved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poKK1f22rcUd"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, k, figsize=(4*k, 4), constrained_layout=True)\n",
        "\n",
        "# Make sure axes is iterable when k = 1\n",
        "if k == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, idx, date in zip(axes, analogue_idx, analogue_dates):\n",
        "    im = ax.imshow(y_train[idx], origin=\"upper\")\n",
        "    ax.set_title(np.datetime_as_string(date, unit='D'))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    cbar0 = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar0.set_label(\"ET [mm/day]\")\n",
        "\n",
        "plt.suptitle(\"ET maps of selected analogue days\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2l77Dk2rcl2"
      },
      "source": [
        "### 2.6.5 Reconstructing ET for the target day\n",
        "\n",
        "The ET map for the target day is reconstructed as a distance-weighted combination of the analogue ET maps.\n",
        "\n",
        "In addition to the reconstructed ET, we compute an _analogue-based uncertainty_, defined as the weighted standard deviation of the analogue ET maps around the reconstructed mean. This quantity reflects the level of agreement among the selected analogues.\n",
        "\n",
        "We then plot:\n",
        "- The reference (observed) ET map\n",
        "- The reconstructed ET map\n",
        "- The reconstruction error map (prediction − reference)\n",
        "- The analogue-based uncertainty map\n",
        "\n",
        "This visual comparison demonstrates how the analogue method transfers spatial patterns from historical situations to the target day. In the error map, high values show overestimation in the generated map, while low values show underestimation. The uncertainty map highlights regions where the analogues disagree, indicating lower confidence in the reconstruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCXqanJtri7I"
      },
      "outputs": [],
      "source": [
        "# --- Distance-based weights (inverse-distance weighting) ---\n",
        "eps = np.finfo(np.float64).eps\n",
        "d = distances[analogue_idx].copy()\n",
        "d[d == 0.0] += eps\n",
        "\n",
        "weights = 1.0 / d\n",
        "weights /= weights.sum()\n",
        "\n",
        "# --- Reconstruct ET ---\n",
        "y_reconstructed = np.zeros_like(y_target_ref)\n",
        "for w, idx in zip(weights, analogue_idx):\n",
        "    y_reconstructed += w * y_train[idx]\n",
        "\n",
        "# --- Error map ---\n",
        "error = y_reconstructed - y_target_ref\n",
        "err_abs_max = np.nanmax(np.abs(error))\n",
        "\n",
        "# --- Analogue-based uncertainty (weighted spread) ---\n",
        "y_var = np.zeros_like(y_target_ref)\n",
        "\n",
        "for w, idx in zip(weights, analogue_idx):\n",
        "    y_var += w * (y_train[idx] - y_reconstructed) ** 2\n",
        "\n",
        "y_uncertainty = np.sqrt(y_var)\n",
        "\n",
        "# --- Plot reference, reconstruction, and error ---\n",
        "vmin = np.nanmin([y_reconstructed, y_target_ref])\n",
        "vmax = np.nanmax([y_reconstructed, y_target_ref])\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 4), constrained_layout=True)\n",
        "\n",
        "# Reference ET\n",
        "im0 = axes[0].imshow(y_target_ref, origin=\"upper\", vmin=vmin, vmax=vmax)\n",
        "axes[0].set_title(\"Reference ET\")\n",
        "axes[0].set_xticks([])\n",
        "axes[0].set_yticks([])\n",
        "cbar0 = plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "cbar0.set_label(\"ET [mm/day]\")\n",
        "\n",
        "# Reconstructed ET\n",
        "im1 = axes[1].imshow(y_reconstructed, origin=\"upper\", vmin=vmin, vmax=vmax)\n",
        "axes[1].set_title(\"Reconstructed ET\")\n",
        "axes[1].set_xticks([])\n",
        "axes[1].set_yticks([])\n",
        "cbar1 = plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "cbar1.set_label(\"ET [mm/day]\")\n",
        "\n",
        "# Error map\n",
        "im2 = axes[2].imshow(\n",
        "    error,\n",
        "    origin=\"upper\",\n",
        "    cmap=\"coolwarm\",\n",
        "    vmin=-err_abs_max,\n",
        "    vmax=err_abs_max,\n",
        ")\n",
        "axes[2].set_title(\"Reconstruction Error (Pred − Ref)\")\n",
        "axes[2].set_xticks([])\n",
        "axes[2].set_yticks([])\n",
        "cbar2 = plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "cbar2.set_label(\"Error [mm/day]\")\n",
        "\n",
        "im3 = axes[3].imshow(\n",
        "    y_uncertainty,\n",
        "    origin=\"upper\",\n",
        "    cmap=\"magma\"\n",
        ")\n",
        "axes[3].set_title(\"Uncertainty (analogue spread)\")\n",
        "axes[3].set_xticks([]); axes[2].set_yticks([])\n",
        "cbar3 = plt.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04)\n",
        "cbar3.set_label(\"mm/day\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqY6Onx0rjbY"
      },
      "source": [
        "### 2.6.6 Quantitative evaluation for the single day\n",
        "\n",
        "To complement visual inspection, we compute the spatial RMSE between:\n",
        "- The reconstructed ET map\n",
        "- The reference ET map\n",
        "\n",
        "RMSE $= \\sqrt{\\frac{1}{N}\\sum^{N}_{i=1}\\left(y_{obs} - y_{pred}\\right)^2}$\n",
        "\n",
        "This provides a single quantitative score summarizing reconstruction accuracy for the example day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3h0mG_5rtTk"
      },
      "outputs": [],
      "source": [
        "# --- RMSE for the single reconstructed day ---\n",
        "rmse_single = np.sqrt(\n",
        "    np.nanmean((y_target_ref - y_reconstructed) ** 2)\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"RMSE for {np.datetime_as_string(target_date, unit='D')}: \"\n",
        "    f\"{rmse_single:.3f} mm/day\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGIxMuPoIdZD"
      },
      "source": [
        "## 2.7 - kNN prediction for the full test period (2018)\n",
        "\n",
        "After illustrating the analogue method for a single day, we now apply the exact same approach to the entire test period.\n",
        "\n",
        "The procedure is identical:\n",
        "- For each test day, kNN identifies climate analogues in the training set.\n",
        "- ET is reconstructed from the analogue ET maps.\n",
        "- Predictions are generated independently for each day.\n",
        "\n",
        "In addition to the mean ET prediction, we also compute an analogue-based uncertainty for each grid cell and day:\n",
        "- For each query day, the `k_neighbors` closest analogues are weighted by similarity (inverse distance).\n",
        "- The predicted ET is the weighted mean of these analogues.\n",
        "- The \"uncertainty\" is the weighted standard deviation of the analogue ET values around the mean.\n",
        "\n",
        "> **Note:** This uncertainty reflects the spread of the analogue ensemble not a formal statistical confidence interval.  \n",
        "> It indicates regions or days where the selected analogues are less consistent (larger spread = higher analogue-based uncertainty), not a formal statistical confidence interval.\n",
        "\n",
        "Here, we call the function to run the kNN with *k = 20*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJBWPtbXyXkt"
      },
      "outputs": [],
      "source": [
        "y_test_mean, y_test_std = knn_predict_with_uncertainty(\n",
        "    X_train     = X_train,\n",
        "    y_train     = y_train,\n",
        "    X_query     = X_test,\n",
        "    k_neighbors = 20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edT6rfWrbJkf"
      },
      "source": [
        "## 2.8 - Visualizing Observed vs Predicted Maps\n",
        "\n",
        "To inspect model performance, we plot observed and predicted ET side by side for selected dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jt2kYt4hF0"
      },
      "outputs": [],
      "source": [
        "target_var = list(et.data_vars)[0]\n",
        "et_da      = et[target_var]\n",
        "lat        = et_da[\"lat\"].values\n",
        "lon        = et_da[\"lon\"].values\n",
        "\n",
        "indices = [0, 50, 100] # <---------------------------------- CHOOSE DAYS TO VISUALISE\n",
        "\n",
        "plot_maps(\n",
        "    y_obs   = y_test,\n",
        "    y_mean  = y_test_mean,\n",
        "    y_std   = y_test_std,\n",
        "    dates   = dates_test,\n",
        "    lon     = lon,\n",
        "    lat     = lat,\n",
        "    indices = indices\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJsf9Rqcbyl2"
      },
      "source": [
        "## 2.9 - Quantitative evaluation using RMSE\n",
        "\n",
        "While visual inspection is useful, quantitative metrics provide a more rigorous assessment.\n",
        "\n",
        "Here, we compute Root Mean Squared Error (RMSE):\n",
        "\n",
        "RMSE $= \\sqrt{\\frac{1}{N}\\sum^{N}_{i=1}\\left(y_{obs} - y_{pred}\\right)^2}$\n",
        "\n",
        "Computed spatially per time step.\n",
        "\n",
        "Can also be aggregated over the entire test period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFoOzgukbukR"
      },
      "outputs": [],
      "source": [
        "rmse_test, segments = compute_rmse(y_test, y_test_mean, dates_test)\n",
        "\n",
        "for i, seg in enumerate(segments, 1):\n",
        "    mean_rmse = np.nanmean(rmse_test[seg])\n",
        "    year = year = str(dates_test[seg][0])[:4]\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(dates_test[seg], rmse_test[seg], '-')\n",
        "    plt.axhline(mean_rmse, color='red', linestyle='--', label=f\"Mean RMSE = {mean_rmse:.2f}\")\n",
        "    plt.legend()\n",
        "    plt.ylim(0)\n",
        "    plt.ylabel(\"RMSE [mm/day]\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.title(f\"RMSE - {year}\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BLlAMDWBoW8"
      },
      "source": [
        "## 2.10 – Sensitivity to the number of neighbours (*k*)\n",
        "\n",
        "Up to this point, the k-nearest neighbours (kNN) model has been evaluated using a fixed number of neighbours.  \n",
        "However, the choice of *k* directly controls the trade-off between local similarity and smoothing:\n",
        "\n",
        "- Small *k*\n",
        "  - Strongly local analogues  \n",
        "  - Potentially sharper spatial patterns  \n",
        "  - Higher sensitivity to noise and outliers  \n",
        "\n",
        "- Large *k*\n",
        "  - More spatial and temporal smoothing  \n",
        "  - Reduced variance and noise  \n",
        "  - Possible loss of fine-scale extremes  \n",
        "\n",
        "To better understand this behaviour, we now **vary the number of neighbours (*k*)** and analyse its impact on:\n",
        "\n",
        "1. **Reconstructed evapotranspiration (ET) maps**\n",
        "2. **Associated uncertainty**, estimated from the analogue spread\n",
        "3. **Temporal RMSE** over the validation period\n",
        "\n",
        "For each value of *k*, we:\n",
        "- Reconstruct ET for the validation period\n",
        "- Compute uncertainty maps from the ensemble of selected analogues\n",
        "- Visually compare observed, predicted, error, and uncertainty fields\n",
        "- Evaluate prediction skill using the RMSE time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEDYPVPLCAIu"
      },
      "outputs": [],
      "source": [
        "y_test_mean, y_test_std = knn_predict_with_uncertainty(\n",
        "    X_train     = X_train,\n",
        "    y_train     = y_train,\n",
        "    X_query     = X_test,\n",
        "    k_neighbors = 5             # <--------------------------------- CHANGE K HERE\n",
        ")\n",
        "\n",
        "indices = [0, 50, 100]          # <--------------------------------- CHANGE MAPS TO VISUALISE HERE\n",
        "\n",
        "plot_maps(\n",
        "    y_obs   = y_test,\n",
        "    y_mean  = y_test_mean,\n",
        "    y_std   = y_test_std,\n",
        "    dates   = dates_test,\n",
        "    lon     = lon,\n",
        "    lat     = lat,\n",
        "    indices = indices\n",
        ")\n",
        "\n",
        "rmse_test, segments = compute_rmse(y_test, y_test_mean, dates_test)\n",
        "\n",
        "for i, seg in enumerate(segments, 1):\n",
        "    mean_rmse = np.nanmean(rmse_test[seg])\n",
        "    year = year = str(dates_test[seg][0])[:4]\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(dates_test[seg], rmse_test[seg], '-')\n",
        "    plt.axhline(mean_rmse, color='red', linestyle='--', label=f\"Mean RMSE = {mean_rmse:.2f}\")\n",
        "    plt.legend()\n",
        "    plt.ylim(0)\n",
        "    plt.ylabel(\"RMSE [mm/day]\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.title(f\"RMSE - {year}\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZxR9STa2XaQi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}