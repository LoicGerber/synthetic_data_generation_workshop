{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd5U49YmCvZC"
      },
      "source": [
        "# Workshop Context and Objectives\n",
        "\n",
        "This notebook is a hands-on workshop exercise demonstrating how to build a workflow from remote sensing and climate data to estimating missing spatio-temporal information using an analogue approach.\n",
        "\n",
        "## Context\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "Suppose we want to run a fully distributed hydrological model over the Volta River Basin. Such models require continuous daily inputs over long time periods, but in many regions observational datasets are incomplete, temporally sparse, or unavailable for recent years. To overcome this limitation, we will use a data-driven analogue method to generate the missing information needed to run the model.\n",
        "\n",
        "<div style=\"display:flex; gap:10px;\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LoicGerber/synthetic_data_generation_workshop/52acd5850ad2311637ecf5704099c15e947f975e/isohyets.png\" width=\"35%\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/LoicGerber/synthetic_data_generation_workshop/52acd5850ad2311637ecf5704099c15e947f975e/dem.png\" width=\"34.8%\">\n",
        "\n",
        "</div>\n",
        "\n",
        "In this workshop, we will use GLEAM evapotranspiration data as our target variable. The goal is to learn the relationships between climate conditions and evapotranspiration from this reference dataset, and then generate synthetic images that statistically and spatially resemble GLEAM. To achieve this, we will use ERA5-Land temperature and precipitation as predictor variables, allowing us to reconstruct realistic evapotranspiration fields even when observations are missing.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 1 - Accessing, downloading, and visualizing data\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "In the first part, we focus on data acquisition and preprocessing:\n",
        "- Access remote sensing and reanalysis data directly from Google Earth Engine (GEE).\n",
        "- Download Actual Evapotranspiration (AET) from the WaPOR Level 1 product (FAO).\n",
        "- Download daily climate predictors (precipitation and temperature) from ERA5-Land.\n",
        "- Spatially subset all datasets to the Volta River Basin using HydroSHEDS geometries.\n",
        "- Convert GEE ImageCollections into local NumPy arrays for simple python handling.\n",
        "- Save and reload processed datasets efficiently using compressed .npz files for reproducibility.\n",
        "- Visualize AET, precipitation, and temperature maps for qualitative inspection.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 2 – kNN-based analogue modeling: method understanding and validation\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "This second part focuses on understanding the kNN-based analogue method through a simple, step-by-step demonstration. Following this illustrative example, the method is applied to reconstruct three full years of daily data within a validation framework, allowing for qualitative and quantitative assessment of its performance.\n",
        "- Load pre-processed NetCDF datasets of ET (target), precipitation, and temperature (predictors) clipped to the Volta Basin.\n",
        "- Ensure temporal alignment of predictors and target.\n",
        "- Optionally subset the spatial domain for faster computation.\n",
        "- Construct covariate features, including lagged predictors (*climate window*), to capture temporal dynamics.\n",
        "- Split the dataset into training and testing periods based on years.\n",
        "- Apply k-nearest neighbors (kNN) regression, where each unobserved ET image is estimated by finding the most similar historical climate “analogues.”\n",
        "- Visualize predicted versus observed ET maps for qualitative assessment.\n",
        "- Perform quantitative evaluation using Root Mean Squared Error (RMSE) over space and time.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Part 3 – kNN-based analogue modeling: production run for 2021–2025\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "In this final part, we move from method validation to a full production run, applying the kNN-based analogue approach to estimate daily ET for the period 2021–2025, using all available historical observations up to 2020 as training data.\n",
        "\n",
        "Key steps include:\n",
        "- Prepare training datasets from ET (target) and climate predictors (precipitation and temperature) for all available historical data.\n",
        "- Apply the kNN analogue method to generate daily ET estimates for 2021–2025.\n",
        "- Compute _analogue-based uncertainty_ for each generated day, defined as the weighted standard deviation across the k nearest analogues.\n",
        "- Visualize daily ET reconstructions and their uncertainty.\n",
        "- Save the production datasets in both NetCDF and compressed `.npz` formats.\n",
        "- Copy final outputs to Google Drive for long-term storage and reproducibility.\n",
        "\n",
        "</details>\n",
        "\n",
        "## Learning Outcomes\n",
        "\n",
        "<details>\n",
        "<summary><b>Details</b></summary>\n",
        "\n",
        "By the end of this workshop, participants will be able to:\n",
        "1. Access and process geospatial datasets from GEE and local NetCDF files.\n",
        "2. Handle spatio-temporal data using `NumPy` and `xarray` efficiently.\n",
        "3. Apply masking and subsetting to focus on a specific river basin.\n",
        "4. Understand and implement a kNN-based analogue approach for estimating missing or unobserved images.\n",
        "5. Validate analogue-based reconstructions using visual diagnostics and RMSE.\n",
        "6. Apply the same method in a production setting to temporally disaggregate remote sensing products.\n",
        "7. Build reproducible workflows that integrate data acquisition, preprocessing, modeling, validation, and production runs.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq2r86o_cHqH"
      },
      "source": [
        "___\n",
        "\n",
        "## Part 3 – Production run: generating ET for 2021–2025\n",
        "\n",
        "In this final part, we run the analogue-based kNN model in production mode.\n",
        "\n",
        "All available observations up to the end of 2020 are used for training, and\n",
        "ET is generated for the period 2021–2025 using climate predictors only.\n",
        "\n",
        "The methodology, predictors, distance metric, and kNN configuration are\n",
        "identical to Part 2.  \n",
        "The only difference is that no validation is performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_IIG7x5cGln"
      },
      "source": [
        "## Import libraries and define helper functions\n",
        "\n",
        "> **Note:** this block must be run, but we will not look into it in details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08-RBxPbcY0L"
      },
      "source": [
        "In this first step, we import all Python libraries required throughout the notebook. Key libraries include:\n",
        "- `NumPy` and `xarray` for numerical data handling\n",
        "- `matplotlib` for visualization\n",
        "- `scikit-learn` for kNN regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuqDZSK1CKqM"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from google.colab import files, drive\n",
        "import matplotlib.pyplot as plt\n",
        "import xarray as xr\n",
        "import gdown\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0zwcnqdL9F"
      },
      "source": [
        "### Prepare production data\n",
        "We define a function `prepare_production_data` to:\n",
        "- Construct covariates (precipitation and temperature)\n",
        "- Optionally include lagged values (`time_window`)\n",
        "- Split the dataset into training and production sets based on availability of the target dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8myT6oUdMXG"
      },
      "outputs": [],
      "source": [
        "def prepare_production_data(et_ds, pre_ds, tmax_ds, time_window):\n",
        "    \"\"\"\n",
        "    Prepare X_train, y_train, X_prod, dates_train, dates_prod\n",
        "    for production runs where ET is unavailable in the future.\n",
        "\n",
        "    Training uses all available ET.\n",
        "    Production uses predictors only, after the last ET date.\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # 1) EXTRACT VARIABLE NAMES FROM DATASETS\n",
        "    # ==========================================================\n",
        "    # Assumes each dataset contains only one variable\n",
        "    target_var = list(et_ds.data_vars)[0]\n",
        "    pre_var    = list(pre_ds.data_vars)[0]\n",
        "    tmax_var   = list(tmax_ds.data_vars)[0]\n",
        "\n",
        "    # Extract DataArrays\n",
        "    et_da   = et_ds[target_var]\n",
        "    pre_da  = pre_ds[pre_var]\n",
        "    tmax_da = tmax_ds[tmax_var]\n",
        "\n",
        "    time_dim = \"time\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # 2) SPLIT TIME INTO TRAINING AND PRODUCTION PERIODS\n",
        "    # ==========================================================\n",
        "    # Find last available date for ET observations\n",
        "    last_et_time = et_da[time_dim].max().values\n",
        "\n",
        "    # Training predictors → same period as ET\n",
        "    pre_train  = pre_da.sel(time=slice(None, last_et_time))\n",
        "    tmax_train = tmax_da.sel(time=slice(None, last_et_time))\n",
        "\n",
        "    # Production predictors → future period (after ET ends)\n",
        "    pre_prod   = pre_da.sel(time=slice(last_et_time, None))\n",
        "    tmax_prod  = tmax_da.sel(time=slice(last_et_time, None))\n",
        "\n",
        "    # ==========================================================\n",
        "    # 3) BUILD FEATURE STACK WITH LAGGED VARIABLES\n",
        "    # ==========================================================\n",
        "    # This inner function builds predictors with time lags\n",
        "    def build_features(pre_da, tmax_da):\n",
        "\n",
        "        feat_list = []\n",
        "\n",
        "        # Loop through predictors\n",
        "        for da in (pre_da, tmax_da):\n",
        "\n",
        "            # Current day predictor\n",
        "            feat_list.append(da)\n",
        "\n",
        "            # Add lagged predictors (t-1, t-2, ..., t-n)\n",
        "            for lag in range(1, time_window + 1):\n",
        "                feat_list.append(\n",
        "                    da.shift({time_dim: lag})\n",
        "                )\n",
        "\n",
        "        # Concatenate all predictors into a single feature dimension\n",
        "        features = xr.concat(feat_list, dim=\"feature\")\n",
        "\n",
        "        # Remove first days where lagged data is incomplete\n",
        "        features = features.isel({time_dim: slice(time_window, None)})\n",
        "\n",
        "        return features\n",
        "\n",
        "    # Build training and production predictor stacks\n",
        "    X_train_da = build_features(pre_train, tmax_train)\n",
        "    X_prod_da  = build_features(pre_prod,  tmax_prod)\n",
        "\n",
        "    # ==========================================================\n",
        "    # 4) ALIGN ET TARGET WITH TRAINING FEATURES\n",
        "    # ==========================================================\n",
        "    # Remove first days so ET matches lagged predictors\n",
        "    et_eff = et_da.isel(time=slice(time_window, None))\n",
        "\n",
        "    # ==========================================================\n",
        "    # 5) IDENTIFY SPATIAL DIMENSIONS\n",
        "    # ==========================================================\n",
        "    # Everything except time is considered spatial\n",
        "    spatial_dims = [d for d in et_eff.dims if d != time_dim]\n",
        "\n",
        "    # ==========================================================\n",
        "    # 6) CONVERT XARRAY DATA → NUMPY ARRAYS\n",
        "    # ==========================================================\n",
        "    # Format predictors for ML:\n",
        "    # (time, features, lat, lon)\n",
        "    X_train = X_train_da.transpose(\n",
        "        time_dim, \"feature\", *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    X_prod = X_prod_da.transpose(\n",
        "        time_dim, \"feature\", *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    # Format target ET:\n",
        "    # (time, lat, lon)\n",
        "    y_train = et_eff.transpose(\n",
        "        time_dim, *spatial_dims\n",
        "    ).values\n",
        "\n",
        "    # ==========================================================\n",
        "    # 7) EXTRACT DATE ARRAYS\n",
        "    # ==========================================================\n",
        "    dates_train = et_eff[time_dim].values\n",
        "    dates_prod  = X_prod_da[time_dim].values\n",
        "\n",
        "    # ==========================================================\n",
        "    # RETURN RESULTS\n",
        "    # ==========================================================\n",
        "    return X_train, y_train, X_prod, dates_train, dates_prod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHw0ihUkd14V"
      },
      "source": [
        "### kNN estimation\n",
        "\n",
        "- For each test day, kNN identifies climate analogues in the training set.\n",
        "- ET is reconstructed from the analogue ET maps.\n",
        "- Predictions are generated independently for each day.\n",
        "- For each query day, the `k_neighbors` closest analogues are weighted by similarity (inverse distance).\n",
        "- The predicted ET is the weighted mean of these analogues.\n",
        "- The \"uncertainty\" is the weighted standard deviation of the analogue ET values around the mean.\n",
        "\n",
        "> **Note:** This uncertainty reflects the spread of the analogue ensemble not a formal statistical confidence interval.  \n",
        "> It indicates regions or days where the selected analogues are less consistent (larger spread = higher analogue-based uncertainty), not a formal statistical confidence interval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onsZfcaUd971"
      },
      "outputs": [],
      "source": [
        "def knn_predict_with_uncertainty(X_train, y_train, X_query, k_neighbors):\n",
        "    \"\"\"\n",
        "    KNN analogue prediction with uncertainty from analogue spread.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : (T_train, n_features, latX, lonX)\n",
        "        Predictor variables for training period\n",
        "    y_train : (T_train, latY, lonY)\n",
        "        Target variable for training period\n",
        "    X_query : (T_query, n_features, latX, lonX)\n",
        "        Predictor variables for query period\n",
        "    k_neighbors : int\n",
        "        Number of analogues (nearest neighbors)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_mean : (T_query, latY, lonY)\n",
        "        Predicted mean field\n",
        "    y_std  : (T_query, latY, lonY)\n",
        "        Analogue-based uncertainty (weighted std)\n",
        "    \"\"\"\n",
        "\n",
        "    # -------------------------------\n",
        "    # Extract dimensions\n",
        "    # -------------------------------\n",
        "    T_train, n_features, n_latX, n_lonX = X_train.shape\n",
        "    T_query = X_query.shape[0]\n",
        "    _, n_latY, n_lonY = y_train.shape\n",
        "\n",
        "    # Total number of spatial pixels\n",
        "    NX = n_latX * n_lonX\n",
        "    NY = n_latY * n_lonY\n",
        "\n",
        "    # ==========================================================\n",
        "    # X MASKING — remove pixels that contain NaNs in ANY time or feature\n",
        "    # ==========================================================\n",
        "\n",
        "    # Combine train + query to ensure consistent valid mask\n",
        "    X_all = np.concatenate([X_train, X_query], axis=0)\n",
        "\n",
        "    # Valid pixel mask → True where all values finite across time + features\n",
        "    valid_X = np.all(np.isfinite(X_all), axis=(0, 1))\n",
        "\n",
        "    # Flatten spatial mask for vector indexing\n",
        "    mask_X = valid_X.ravel()\n",
        "\n",
        "    # Reshape predictors → (time, features, pixels)\n",
        "    # then keep only valid pixels\n",
        "    Xtr = X_train.reshape(T_train, n_features, NX)[:, :, mask_X]\n",
        "    Xq  = X_query.reshape(T_query, n_features, NX)[:, :, mask_X]\n",
        "\n",
        "    # Flatten feature + pixel dimensions into single vector\n",
        "    # final shapes:\n",
        "    #   Xtr_vec = (T_train, n_valid_features)\n",
        "    #   Xq_vec  = (T_query, n_valid_features)\n",
        "    Xtr_vec = Xtr.reshape(T_train, -1)\n",
        "    Xq_vec  = Xq.reshape(T_query,  -1)\n",
        "\n",
        "    # ==========================================================\n",
        "    # Y MASKING — remove pixels invalid in training target\n",
        "    # ==========================================================\n",
        "\n",
        "    # Valid target pixels across ALL training time\n",
        "    valid_Y = np.all(np.isfinite(y_train), axis=0)\n",
        "\n",
        "    # Flatten spatial mask\n",
        "    mask_Y = valid_Y.ravel()\n",
        "\n",
        "    # Reshape y_train → (time, pixels) and keep valid ones\n",
        "    ytr = y_train.reshape(T_train, NY)[:, mask_Y]\n",
        "\n",
        "    # ==========================================================\n",
        "    # NEAREST NEIGHBOUR SEARCH\n",
        "    # ==========================================================\n",
        "\n",
        "    # Build KNN search structure\n",
        "    nn = NearestNeighbors(\n",
        "        n_neighbors=k_neighbors,\n",
        "        metric=\"euclidean\",\n",
        "        n_jobs=-1              # use all CPU cores\n",
        "    )\n",
        "\n",
        "    # Fit on training predictors\n",
        "    nn.fit(Xtr_vec)\n",
        "\n",
        "    # Find k nearest analogues for each query timestep\n",
        "    distances, indices = nn.kneighbors(Xq_vec)\n",
        "\n",
        "    # ==========================================================\n",
        "    # DISTANCE → WEIGHTS\n",
        "    # ==========================================================\n",
        "\n",
        "    # Convert distances to inverse-distance weights\n",
        "    # (small distance → large weight)\n",
        "    weights = 1.0 / (distances + 1e-12)\n",
        "\n",
        "    # Normalize weights so each row sums to 1\n",
        "    weights /= weights.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # ==========================================================\n",
        "    # ANALOGUE PREDICTIONS\n",
        "    # ==========================================================\n",
        "\n",
        "    # Dimensions\n",
        "    Tq, k = indices.shape\n",
        "\n",
        "    # Container for analogue target values\n",
        "    # shape = (query_time, k_neighbors, valid_pixels)\n",
        "    y_pred = np.empty((Tq, k, ytr.shape[1]))\n",
        "\n",
        "    # Gather analogue fields\n",
        "    for i in range(Tq):\n",
        "        # indices[i] → indices of k nearest training dates\n",
        "        y_pred[i] = ytr[indices[i]]\n",
        "\n",
        "    # ==========================================================\n",
        "    # WEIGHTED MEAN PREDICTION\n",
        "    # ==========================================================\n",
        "\n",
        "    # Weighted average across k analogues\n",
        "    y_mean_flat = np.sum(\n",
        "        weights[:, :, None] * y_pred,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # ==========================================================\n",
        "    # UNCERTAINTY ESTIMATION\n",
        "    # ==========================================================\n",
        "\n",
        "    # Weighted variance across analogues\n",
        "    y_var_flat = np.sum(\n",
        "        weights[:, :, None] * (y_pred - y_mean_flat[:, None, :])**2,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Standard deviation = uncertainty estimate\n",
        "    y_std_flat = np.sqrt(y_var_flat)\n",
        "\n",
        "    # ==========================================================\n",
        "    # RESTORE FULL SPATIAL GRID (reinsert masked pixels)\n",
        "    # ==========================================================\n",
        "\n",
        "    # Initialize full grids with NaNs\n",
        "    y_mean = np.full((Tq, NY), np.nan)\n",
        "    y_std  = np.full((Tq, NY), np.nan)\n",
        "\n",
        "    # Fill valid pixels only\n",
        "    y_mean[:, mask_Y] = y_mean_flat\n",
        "    y_std[:,  mask_Y] = y_std_flat\n",
        "\n",
        "    # Reshape back to spatial maps\n",
        "    y_mean = y_mean.reshape(Tq, n_latY, n_lonY)\n",
        "    y_std  = y_std.reshape(Tq, n_latY, n_lonY)\n",
        "\n",
        "    return y_mean, y_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeR2dbqeANJ"
      },
      "source": [
        "### Visualizing Observed vs Predicted Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7TAqVE7eDLv"
      },
      "outputs": [],
      "source": [
        "def plot_maps(y_obs, y_mean, y_std, dates, lon, lat, indices):\n",
        "    \"\"\"\n",
        "    Plot observed ET, predicted ET, error, and uncertainty maps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_obs : array (T, lat, lon)\n",
        "        Observed / reference ET (use None for production runs).\n",
        "    y_mean : array (T, lat, lon)\n",
        "        Mean reconstructed ET.\n",
        "    y_std : array (T, lat, lon)\n",
        "        Uncertainty (analogue spread).\n",
        "    dates : array-like (T,)\n",
        "        Datetime array.\n",
        "    lon, lat : 1D or 2D arrays\n",
        "        Spatial coordinates.\n",
        "    indices : list of int\n",
        "        Time indices to visualise.\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # PREPARE SPATIAL GRID\n",
        "    # ==========================================================\n",
        "    # If coordinates are 1D vectors, convert to 2D meshgrid\n",
        "    # so they match the shape required by pcolormesh.\n",
        "    if lon.ndim == 1 and lat.ndim == 1:\n",
        "        Lon, Lat = np.meshgrid(lon, lat)\n",
        "    else:\n",
        "        # Already 2D coordinate grids\n",
        "        Lon, Lat = lon, lat\n",
        "\n",
        "    # ==========================================================\n",
        "    # LOOP THROUGH REQUESTED TIME INDICES\n",
        "    # ==========================================================\n",
        "    for idx in indices:\n",
        "\n",
        "        # Extract prediction and uncertainty for this timestep\n",
        "        y_pred = y_mean[idx]\n",
        "        y_unc  = y_std[idx]\n",
        "\n",
        "        # Convert date to readable format\n",
        "        date = np.array(dates[idx]).astype(\"datetime64[D]\")\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # IF REFERENCE DATA EXISTS → compute error + shared scale\n",
        "        # ------------------------------------------------------\n",
        "        if y_obs is not None:\n",
        "\n",
        "            # Reference map\n",
        "            y_ref = y_obs[idx]\n",
        "\n",
        "            # Prediction error map\n",
        "            y_err = y_pred - y_ref\n",
        "\n",
        "            # Use same color scale for observed + predicted\n",
        "            vmin  = np.nanmin([y_ref, y_pred])\n",
        "            vmax  = np.nanmax([y_ref, y_pred])\n",
        "\n",
        "            # Error scale based on robust percentile\n",
        "            vmax_err = np.nanpercentile(np.abs(y_err), 95)\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # IF NO REFERENCE DATA (production mode)\n",
        "        # ------------------------------------------------------\n",
        "        else:\n",
        "            y_ref = None\n",
        "            y_err = None\n",
        "\n",
        "            # Scale only based on prediction\n",
        "            vmin  = np.nanmin(y_pred)\n",
        "            vmax  = np.nanmax(y_pred)\n",
        "\n",
        "        # Robust scale for uncertainty\n",
        "        vmax_std = np.nanpercentile(y_unc, 95)\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        # DETERMINE NUMBER OF PANELS\n",
        "        # ------------------------------------------------------\n",
        "        # If reference exists → show 4 panels\n",
        "        # Otherwise → only prediction + uncertainty\n",
        "        ncols = 4 if y_obs is not None else 2\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(\n",
        "            1, ncols,\n",
        "            figsize=(4.5 * ncols, 4),\n",
        "            constrained_layout=True\n",
        "        )\n",
        "\n",
        "        # Column pointer for flexible plotting\n",
        "        col = 0\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 1 — OBSERVED MAP\n",
        "        # ======================================================\n",
        "        if y_obs is not None:\n",
        "\n",
        "            im0 = axes[col].pcolormesh(\n",
        "                Lon, Lat, y_ref,\n",
        "                shading=\"auto\",\n",
        "                vmin=vmin, vmax=vmax\n",
        "            )\n",
        "\n",
        "            axes[col].set_title(f\"Observed ET\\n{date}\")\n",
        "            axes[col].set_aspect(\"equal\")\n",
        "\n",
        "            # Colorbar\n",
        "            c0 = plt.colorbar(im0, ax=axes[col])\n",
        "            c0.set_label(\"[mm/day]\")\n",
        "\n",
        "            col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 2 — PREDICTED MAP\n",
        "        # ======================================================\n",
        "        im1 = axes[col].pcolormesh(\n",
        "            Lon, Lat, y_pred,\n",
        "            shading=\"auto\",\n",
        "            vmin=vmin, vmax=vmax\n",
        "        )\n",
        "\n",
        "        # Title depends on mode\n",
        "        if y_obs is not None:\n",
        "            axes[col].set_title(\"Predicted ET\")\n",
        "        else:\n",
        "            axes[col].set_title(f\"Predicted ET\\n{date}\")\n",
        "\n",
        "        axes[col].set_aspect(\"equal\")\n",
        "\n",
        "        # Colorbar\n",
        "        c1 = plt.colorbar(im1, ax=axes[col])\n",
        "        c1.set_label(\"[mm/day]\")\n",
        "\n",
        "        col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # PANEL 3 — ERROR MAP (only if reference exists)\n",
        "        # ======================================================\n",
        "        if y_obs is not None:\n",
        "\n",
        "            im2 = axes[col].pcolormesh(\n",
        "                Lon, Lat, y_err,\n",
        "                shading=\"auto\",\n",
        "                vmin=-vmax_err, vmax=vmax_err,\n",
        "                cmap=\"coolwarm\"   # diverging colormap for errors\n",
        "            )\n",
        "\n",
        "            axes[col].set_title(\"Error (Pred − Ref)\")\n",
        "            axes[col].set_aspect(\"equal\")\n",
        "\n",
        "            # Colorbar\n",
        "            c2 = plt.colorbar(im2, ax=axes[col])\n",
        "            c2.set_label(\"[mm/day]\")\n",
        "\n",
        "            col += 1\n",
        "\n",
        "        # ======================================================\n",
        "        # FINAL PANEL — UNCERTAINTY MAP\n",
        "        # ======================================================\n",
        "        im3 = axes[col].pcolormesh(\n",
        "            Lon, Lat, y_unc,\n",
        "            shading=\"auto\",\n",
        "            vmin=0, vmax=vmax_std,\n",
        "            cmap=\"magma\"      # sequential colormap for uncertainty\n",
        "        )\n",
        "\n",
        "        axes[col].set_title(\"Uncertainty (analogue spread)\")\n",
        "        axes[col].set_aspect(\"equal\")\n",
        "\n",
        "        # Colorbar\n",
        "        c3 = plt.colorbar(im3, ax=axes[col])\n",
        "        c3.set_label(\"[mm/day]\")\n",
        "\n",
        "        # ======================================================\n",
        "        # DISPLAY FIGURE\n",
        "        # ======================================================\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMOJXKZxeFU4"
      },
      "source": [
        "### Quantitative evaluation using RMSE\n",
        "\n",
        "- Here, we compute Root Mean Squared Error (RMSE):\n",
        "- RMSE $= \\sqrt{\\frac{1}{N}\\sum^{N}_{i=1}\\left(y_{obs} - y_{pred}\\right)^2}$\n",
        "- Computed spatially per time step.\n",
        "- Can also be aggregated over the entire test period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPIgtvkveHov"
      },
      "outputs": [],
      "source": [
        "def compute_rmse(y_obs, y_pred, dates, max_gap_days=1):\n",
        "    \"\"\"\n",
        "    Compute RMSE over space for each time step and split by date gaps.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rmse : (T,)\n",
        "    segments : list of index arrays for continuous date segments\n",
        "    \"\"\"\n",
        "\n",
        "    # ==========================================================\n",
        "    # INITIALIZATION\n",
        "    # ==========================================================\n",
        "    # Number of time steps\n",
        "    T = y_obs.shape[0]\n",
        "\n",
        "    # Array to store RMSE at each timestep\n",
        "    rmse = np.zeros(T)\n",
        "\n",
        "    # ==========================================================\n",
        "    # COMPUTE SPATIAL RMSE PER TIME STEP\n",
        "    # ==========================================================\n",
        "    for t in range(T):\n",
        "\n",
        "        # Difference map between observation and prediction\n",
        "        diff = y_obs[t] - y_pred[t]\n",
        "\n",
        "        # RMSE over spatial domain (ignores NaNs)\n",
        "        rmse[t] = np.sqrt(np.nanmean(diff**2))\n",
        "\n",
        "    # ==========================================================\n",
        "    # DETECT TEMPORAL GAPS IN THE DATE SERIES\n",
        "    # ==========================================================\n",
        "    # Convert dates to numpy array for vectorized operations\n",
        "    dates = np.asarray(dates)\n",
        "\n",
        "    # Compute day differences between consecutive dates\n",
        "    gaps = np.diff(dates).astype(\"timedelta64[D]\").astype(int)\n",
        "\n",
        "    # Identify where gaps exceed allowed threshold\n",
        "    breaks = np.where(gaps > max_gap_days)[0]\n",
        "\n",
        "    # ==========================================================\n",
        "    # SPLIT TIME SERIES INTO CONTINUOUS SEGMENTS\n",
        "    # ==========================================================\n",
        "    segments = []\n",
        "\n",
        "    # Start index of current segment\n",
        "    start = 0\n",
        "\n",
        "    # Loop over detected breaks\n",
        "    for b in breaks:\n",
        "\n",
        "        # Segment runs from current start → break index\n",
        "        segments.append(np.arange(start, b + 1))\n",
        "\n",
        "        # Next segment starts after the break\n",
        "        start = b + 1\n",
        "\n",
        "    # Add final segment after last break\n",
        "    segments.append(np.arange(start, T))\n",
        "\n",
        "    # ==========================================================\n",
        "    # RETURN RESULTS\n",
        "    # ==========================================================\n",
        "    return rmse, segments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6POKKeCoyx-n"
      },
      "source": [
        "## 3.1 - Temporal coverage for production\n",
        "\n",
        "The datasets already loaded from Google Drive contain:\n",
        "- ET (GLEAM): daily, 1980–2020\n",
        "- Precipitation (ERA5-Land): daily, 1950–2025\n",
        "- Temperature (ERA5-Land): daily, 1950–2025\n",
        "\n",
        "This allows us to:\n",
        "- train the model on 2000–2020\n",
        "- generate ET for 2021–2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP-1R8DGOE2r"
      },
      "outputs": [],
      "source": [
        "url_et   = \"https://drive.google.com/file/d/1QKJCwt44LdsQNpNBIfFA4-kFs2MTaJRD/view?usp=drive_link\"\n",
        "url_pre  = \"https://drive.google.com/file/d/1ZyRFuYSFle5zqPpyoSBXLiyIp5frSCWk/view?usp=drive_link\"\n",
        "url_tmax = \"https://drive.google.com/file/d/1Ru0eKa_DrvjA2x_F1MF7EPixge2VXIJh/view?usp=drive_link\"\n",
        "\n",
        "gdown.download(url_et,   output=\"et.nc\",   fuzzy=True)\n",
        "gdown.download(url_pre,  output=\"pre.nc\",  fuzzy=True)\n",
        "gdown.download(url_tmax, output=\"tmax.nc\", fuzzy=True)\n",
        "\n",
        "et   = xr.open_dataset(\"et.nc\")\n",
        "pre  = xr.open_dataset(\"pre.nc\")\n",
        "tmax = xr.open_dataset(\"tmax.nc\")\n",
        "\n",
        "pre  =  pre.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
        "tmax = tmax.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
        "\n",
        "# Extend predictors beyond 2020 for production\n",
        "t_start = \"2010-01-01\"\n",
        "t_end   = \"2025-12-31\"\n",
        "\n",
        "et   =   et.sel(time=slice(t_start, \"2020-12-31\"))   # ET only available until 2020\n",
        "pre  =  pre.sel(time=slice(t_start, t_end))\n",
        "tmax = tmax.sel(time=slice(t_start, t_end))\n",
        "\n",
        "# Ensure strict temporal alignment to ET where needed\n",
        "pre  =  pre.sel(time=pre.time)\n",
        "tmax = tmax.sel(time=tmax.time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz2hyiXgoq9O"
      },
      "source": [
        "## 3.2 - Preparing training and production datasets\n",
        "\n",
        "We reuse the same `prepare_data` logic as in Part 2, but slightly adapted to the production run.\n",
        "The last five years (2021–2025) are treated as the **test block**, which now\n",
        "corresponds to the production period.\n",
        "\n",
        "We prepare:\n",
        "- `X_train` and `y_train` from all available historical ET\n",
        "- `X_prod` from predictors only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR50-eH9q6ig"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_prod, dates_train, dates_prod = prepare_production_data(\n",
        "    et_ds       = et,\n",
        "    pre_ds      = pre,\n",
        "    tmax_ds     = tmax,\n",
        "    time_window = 2\n",
        ")\n",
        "\n",
        "print(\"Training:\", dates_train[0], \"-\", dates_train[-1])\n",
        "print(\"Production:\", dates_prod[0], \"-\", dates_prod[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xvr1YKM554u"
      },
      "source": [
        "## 3.3 - kNN analogue prediction with uncertainty\n",
        "\n",
        "We now run the kNN-based analogue reconstruction.  \n",
        "\n",
        "- The mean ET is the weighted average of the `k_neighbors` closest analogues.\n",
        "- The uncertainty is the weighted standard deviation of the analogue ET values.\n",
        "\n",
        "> This uncertainty reflects the spread among analogues not a formal statistical confidence interval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgOf5zK7t96u"
      },
      "outputs": [],
      "source": [
        "y_prod_mean, y_prod_std = knn_predict_with_uncertainty(\n",
        "    X_train     = X_train,\n",
        "    y_train     = y_train,\n",
        "    X_query     = X_prod,\n",
        "    k_neighbors = 20\n",
        ")\n",
        "\n",
        "print(\"Production ET generated for:\")\n",
        "print(dates_prod[0], \"-\", dates_prod[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz3mhms46HvW"
      },
      "source": [
        "## 3.4 - Visualising production ET and uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6NgWXZCuO__"
      },
      "outputs": [],
      "source": [
        "target_var = list(et.data_vars)[0]\n",
        "et_da      = et[target_var]\n",
        "lat        = et_da[\"lat\"].values\n",
        "lon        = et_da[\"lon\"].values\n",
        "\n",
        "plot_maps(\n",
        "    y_obs   = None,\n",
        "    y_mean  = y_prod_mean,\n",
        "    y_std   = y_prod_std,\n",
        "    dates   = dates_prod,\n",
        "    lon     = lon,\n",
        "    lat     = lat,\n",
        "    indices = [0, 120, 365] # <------------------------------------ CHANGE DAYS TO VISUALIZE HERE (based on index)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwDurEcu6UXo"
      },
      "source": [
        "We also examine temporal and spatial patterns of uncertainty:\n",
        "- Spatially averaged uncertainty per day\n",
        "- Mean uncertainty map over 2021–2025\n",
        "- Distribution of ET uncertainty values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMR1tTuxvX2A"
      },
      "outputs": [],
      "source": [
        "# Spatial mean uncertainty per day\n",
        "uncertainty_time_mean = np.nanmean(y_prod_std, axis=(1, 2))\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(dates_prod, uncertainty_time_mean, lw=1.5)\n",
        "plt.ylabel(\"Mean uncertainty [mm/day]\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.title(\"Spatially averaged ET uncertainty\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mean uncertainty map\n",
        "mean_uncertainty_map = np.nanmean(y_prod_std, axis=0)\n",
        "plt.figure(figsize=(5, 4))\n",
        "im = plt.pcolormesh(lon, lat, mean_uncertainty_map, shading=\"auto\", cmap=\"inferno\")\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.title(\"Mean ET uncertainty\")\n",
        "plt.xticks([]); plt.yticks([])\n",
        "c = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "c.set_label(\"Mean uncertainty [mm/day]\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Histogram of uncertainty\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.hist(y_prod_std.ravel(), bins=50, density=True, alpha=0.8)\n",
        "plt.xlabel(\"Uncertainty [mm/day]\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Distribution of ET uncertainty\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3DkDkWs6bst"
      },
      "source": [
        "## 3.5 - Saving production ET and uncertainty\n",
        "\n",
        "We save the results as:\n",
        "- NetCDF (`et_mean` and `et_uncertainty`)\n",
        "- Compressed `.npz` for lightweight local usage\n",
        "\n",
        "Both files are copied to Google Drive for persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjwWOzPcvt6L"
      },
      "outputs": [],
      "source": [
        "# Save NetCDF\n",
        "ds_prod = xr.Dataset(\n",
        "    data_vars=dict(\n",
        "        et_mean=((\"time\", \"lat\", \"lon\"), y_prod_mean),\n",
        "        et_uncertainty=((\"time\", \"lat\", \"lon\"), y_prod_std),\n",
        "    ),\n",
        "    coords=dict(time=dates_prod, lat=lat, lon=lon),\n",
        "    attrs=dict(\n",
        "        title=\"Analogue-based evapotranspiration production run\",\n",
        "        method=\"kNN climate analogue reconstruction\",\n",
        "        uncertainty=\"Analogue spread (weighted standard deviation)\",\n",
        "        period=\"2021–2025\",\n",
        "        units=\"mm day-1\",\n",
        "    ),\n",
        ")\n",
        "ds_prod[\"et_mean\"].attrs = dict(\n",
        "    long_name=\"Daily evapotranspiration (mean reconstruction)\",\n",
        "    units=\"mm day-1\",\n",
        "    description=\"Weighted mean of k nearest climate analogues\"\n",
        ")\n",
        "ds_prod[\"et_uncertainty\"].attrs = dict(\n",
        "    long_name=\"Evapotranspiration uncertainty\",\n",
        "    units=\"mm day-1\",\n",
        "    description=\"Weighted standard deviation across climate analogues\"\n",
        ")\n",
        "\n",
        "output_path = \"ET_production_2021_2025_mean_uncertainty.nc\" # <---------------------- CHANGE FILE NAME HERE\n",
        "ds_prod.to_netcdf(output_path)\n",
        "files.download(output_path)\n",
        "\n",
        "# Copy to Drive\n",
        "name      = \"Volta_ET_Production_2021_2025\"       # <-------------------------------- CHANGE FILE NAME HERE\n",
        "save_dir  = \"/content/drive/MyDrive/SyntheticSatDataWorkshop2026\" # <---------------- CHANGE DIRECTORY HERE\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "data_path = os.path.join(save_dir, f\"{name}.nc\")\n",
        "!cp ET_production_2021_2025_mean_uncertainty.nc \"$data_path\"\n",
        "print(f\"Production dataset saved to Google Drive at:\\n{data_path}\")\n",
        "\n",
        "# Save compressed npz\n",
        "np.savez_compressed(\n",
        "    f\"{name}.npz\",\n",
        "    et_mean=y_prod_mean,\n",
        "    et_std=y_prod_std,\n",
        "    dates=dates_prod,\n",
        "    lat=lat,\n",
        "    lon=lon\n",
        ")\n",
        "files.download(f\"{name}.npz\")\n",
        "!cp Volta_ET_Production_2021_2025.npz \"$save_dir\" # <-------------------------------- CHANGE FILE NAME HERE\n",
        "print(f\"Compressed dataset also saved to Google Drive at:\\n{save_dir}/{name}.npz\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6_IIG7x5cGln"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
